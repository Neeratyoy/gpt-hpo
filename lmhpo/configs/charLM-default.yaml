activation: relu
batch_size: 32
block_size: 256
embed_size: 384
learning_rate: 3.0e-5
lr_schedule: cosine
min_learning_rate: 0
n_layers: 3
num_heads: 3
optimizer_name: adamw
warmup_factor: 0.05
wide_factor: 4
dropout: 0.2
prenormalize: true